# Research-to-RAG Pipeline

Automated pipeline to research a topic, extract references, download papers, and convert everything to markdown for RAG.

## Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Research-to-RAG Pipeline                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ 1. Research  â”‚â”€â”€â”€â–¶â”‚ 2. Parse     â”‚â”€â”€â”€â–¶â”‚ 3. Retrieve  â”‚      â”‚
â”‚  â”‚    (Gemini)  â”‚    â”‚    Refs      â”‚    â”‚  & Verify    â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚         â”‚                   â”‚                   â”‚               â”‚
â”‚         â–¼                   â–¼                   â–¼               â”‚
â”‚  research_report.md   references.json    papers/*.pdf          â”‚
â”‚                       - DOIs             bibtex/*.bib          â”‚
â”‚                       - arXiv IDs        verified/*.bib        â”‚
â”‚                       - GitHub repos            â”‚               â”‚
â”‚                       - URLs                    â”‚               â”‚
â”‚                              â”‚                  â”‚               â”‚
â”‚                              â–¼                  â–¼               â”‚
â”‚                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚                        â”‚      4. Ingest           â”‚            â”‚
â”‚                        â”‚   Convert to Markdown    â”‚            â”‚
â”‚                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                    â”‚                            â”‚
â”‚                                    â–¼                            â”‚
â”‚                          4_markdown/                            â”‚
â”‚                          â”œâ”€â”€ papers/                            â”‚
â”‚                          â”œâ”€â”€ github/                            â”‚
â”‚                          â”œâ”€â”€ web/                               â”‚
â”‚                          â””â”€â”€ research/                          â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Requirements

1. **Install dependencies:**
   ```bash
   uv sync --all-extras
   ```

2. **Set up API key** (required for Step 1 - Deep Research):
   
   Option A: Environment variable
   ```bash
   export GEMINI_API_KEY="your-gemini-api-key"
   ```
   
   Option B: Create `.env` file in project root
   ```bash
   # .env
   GEMINI_API_KEY=your-gemini-api-key
   ```

## Usage

### Full Pipeline

```bash
# Research a topic and process all references
uv run python scripts/pipeline_cli.py "Machine learning for drug discovery"

# With custom output directory
uv run python scripts/pipeline_cli.py "Quantum computing" -o ./my_output

# Limit downloads to save time
uv run python scripts/pipeline_cli.py "Topic" --max-papers 5 --max-repos 3 --max-urls 5
```

### Skip Research (Use Existing File)

```bash
# Use an existing research report (skips Step 1)
uv run python scripts/pipeline_cli.py "ML" --research-file path/to/research.md
```

### Options

| Option | Default | Description |
|--------|---------|-------------|
| `topic` | (required) | Research topic |
| `-o, --output` | `pipeline_output` | Output directory |
| `--research-file` | None | Skip research, use existing file |
| `--max-papers` | 10 | Max papers to download |
| `--max-repos` | 5 | Max GitHub repos to clone |
| `--max-urls` | 10 | Max URLs to ingest |

## Output Structure

```
pipeline_output/
â”œâ”€â”€ 1_research/              # Step 1: Deep Research
â”‚   â””â”€â”€ research/
â”‚       â””â”€â”€ research_report.md   # AI-generated research report
â”‚
â”œâ”€â”€ 2_references/            # Step 2: Parsed References
â”‚   â”œâ”€â”€ references.json      # Structured reference data
â”‚   â””â”€â”€ references.md        # Human-readable summary
â”‚
â”œâ”€â”€ 3_papers/                # Step 3: Downloaded Papers
â”‚   â”œâ”€â”€ paper1.pdf
â”‚   â””â”€â”€ paper2.pdf
â”‚
â”œâ”€â”€ 3_bibtex/                # Step 3: BibTeX Citations
â”‚   â”œâ”€â”€ doi_10.1234_xxx.bib  # Individual BibTeX files
â”‚   â”œâ”€â”€ arXiv_2301.12345.bib
â”‚   â”œâ”€â”€ combined.bib         # All citations merged
â”‚   â””â”€â”€ verified/            # BibTeX verification results
â”‚       â”œâ”€â”€ verified.bib     # Valid BibTeX entries
â”‚       â”œâ”€â”€ failed.bib       # Failed entries (if any)
â”‚       â””â”€â”€ report.md        # Verification report
â”‚
â”œâ”€â”€ 4_markdown/              # Step 4: RAG-Ready Markdown
â”‚   â”œâ”€â”€ papers/              # Converted PDFs
â”‚   â”œâ”€â”€ github/              # Cloned repos
â”‚   â”œâ”€â”€ web/                 # Crawled URLs
â”‚   â””â”€â”€ research/            # Research report
â”‚
â””â”€â”€ PIPELINE_REPORT.md       # Summary report
```

## Pipeline Steps

### Step 1: Deep Research

Uses Google Gemini to conduct comprehensive research on the topic.

- **Input:** Topic string
- **Output:** `1_research/research/research_report.md`
- **Requires:** `GEMINI_API_KEY`

### Step 2: Parse References

Extracts references from the research report using regex patterns:

- **DOIs:** `10.xxxx/...` patterns
- **arXiv:** `arXiv:YYMM.NNNNN` or `YYMM.NNNNN` patterns
- **GitHub:** `github.com/owner/repo` patterns
- **URLs:** General HTTP/HTTPS links

### Step 3: Retrieve & DOI2BIB

Downloads papers and generates BibTeX citations using CLI commands:

- `uv run parser retrieve` - Fetches PDFs from arXiv, Unpaywall, PMC, etc.
- `uv run parser doi2bib` - Generates BibTeX using CrossRef, Semantic Scholar
- `uv run parser verify` - Validates and verifies BibTeX entries
- Creates `combined.bib` with all citations
- Creates `verified/` directory with verification results

### Step 4: Ingest to Markdown

Converts all content to markdown using `ingestor` CLI:

- `uv run ingestor ingest` - PDFs â†’ Markdown with extracted text, figures, tables
- `uv run ingestor clone` - GitHub repos â†’ Markdown with code files
- `uv run ingestor ingest` - URLs â†’ Markdown with web content
- Research report â†’ Copied to output

## Example

```bash
$ uv run python scripts/pipeline.py "Transformer architectures in NLP" -o ./nlp_research

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              Research-to-RAG Pipeline                             â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Topic: Transformer architectures in NLP                          â•‘
â•‘  Output: nlp_research                                             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
STEP 1: DEEP RESEARCH
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â–¶ Researching: Transformer architectures in NLP...
âœ… Research report saved: nlp_research/1_research/research_report.md

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
STEP 2: PARSE REFERENCES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  ğŸ“„ DOIs found: 12
  ğŸ“„ arXiv papers: 8
  ğŸ“‚ GitHub repos: 5
  ğŸ”— Other URLs: 15
âœ… References saved: nlp_research/2_references/references.json

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
STEP 3: RETRIEVE PAPERS & DOI2BIB
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  Processing 10 papers...
  ğŸ“– 10.xxxx/nature12373
     âœ“ BibTeX generated
     âœ“ PDF downloaded
...
âœ… Papers downloaded: 8/10
âœ… BibTeX entries: 10
âœ… BibTeX verified: 10/10

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
STEP 4: INGEST TO MARKDOWN
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âœ… Ingestion complete:
   PDFs: 8
   GitHub: 5
   URLs: 10
   Research: 1
   Total markdown files: 24

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    âœ… Pipeline Complete!                          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

## CLI Commands Reference

All commands require `uv run` prefix to use the correct Python environment:

```bash
# Research
uv run researcher research "topic" -o output/

# Paper retrieval
uv run parser retrieve -d DOI -o papers/
uv run parser doi2bib arXiv:XXXX.XXXXX
uv run parser verify combined.bib -o verified/

# Ingestion
uv run ingestor ingest file.pdf -o output/
uv run ingestor clone https://github.com/owner/repo -o output/
```

## Environment Variables

| Variable | Required | Description |
|----------|----------|-------------|
| `GEMINI_API_KEY` | Yes (for Step 1) | Google Gemini API key |
| `GOOGLE_API_KEY` | Alternative | Alternative name for Gemini API key |
| `INGESTOR_EMAIL` | Recommended | Email for CrossRef, Unpaywall, OpenAlex |
| `S2_API_KEY` | Optional | Semantic Scholar API key (higher rate limits) |

## Tips

1. **Skip research for faster iteration:** If you already have a research document, use `--research-file` to skip the AI research step.

2. **Limit downloads for testing:** Use `--max-papers 3 --max-repos 2` when testing to save time.

3. **Check references first:** After Step 2, review `2_references/references.md` to see what will be downloaded.

4. **Use the combined BibTeX:** `3_bibtex/combined.bib` contains all citations ready for LaTeX.

5. **Verify BibTeX entries:** Check `3_bibtex/verified/report.md` for verification details.
